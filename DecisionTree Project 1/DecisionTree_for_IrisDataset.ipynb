{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "old_settings = np.seterr(all='raise') # it lets you know the errors in floating points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "features = iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Store feature names and its corresponding indices initially. Otherwise in every recusive call \n",
    "    we need to make new X ndarray which increases complexity.\n",
    "'''\n",
    "feature_indices = {}\n",
    "for i in range(len(features)):\n",
    "    feature_indices[features[i]] = feature_indices.get(features[i],0) + i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Implementation of Decision Tree class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.level = 0\n",
    "        self.max_gain_ratio_feature = None\n",
    "        self.X_feature_boundry = None # boundry of max_gain_ratio_feature on which we split in two parts\n",
    "        \n",
    "    \n",
    "    # claclate entropy of node having output Y\n",
    "    def entropy_node(self, Y):\n",
    "        total_freq = len(Y)\n",
    "\n",
    "        # store different classes possible with its frequency\n",
    "        class_freq = {}\n",
    "        for c in Y:\n",
    "            class_freq[c] = class_freq.get(c,0) + 1\n",
    "\n",
    "        # Now we calculate entropy or info required\n",
    "        info_req =  0\n",
    "        for k in class_freq:\n",
    "            prob_k = class_freq[k]/total_freq\n",
    "            if prob_k!=0:\n",
    "                info_req += (-1 * prob_k * np.log2(prob_k))\n",
    "        return info_req\n",
    "    \n",
    "    \n",
    "    \n",
    "    # calculate entropy(info required) after spliting on basis of feature having index f_i\n",
    "    def info_fi(self, X, Y, f_i, boundry):\n",
    "        entropy_after_split = np.float(0)\n",
    "        # feature on which we goona split\n",
    "        feature = X[:, f_i] # feature corresponds to index f_i\n",
    "        Y_left = Y[np.where(feature <= boundry)]\n",
    "        Y_right = Y[np.where(feature > boundry)]\n",
    "\n",
    "        if len(Y_left)!=0:\n",
    "            entropy_after_split += ( np.float(len(Y_left)/len(Y)) * self.entropy_node(Y_left) ) \n",
    "        if len(Y_right) !=0 :\n",
    "            entropy_after_split += ( np.float(len(Y_right)/len(Y)) * self.entropy_node(Y_right) )\n",
    "\n",
    "        return entropy_after_split\n",
    "    \n",
    "    \n",
    "    \n",
    "    # calculate split_info of feature having index f_i\n",
    "    def find_split_info(self, X, Y, f_i, boundry):\n",
    "        size = len(Y)\n",
    "        split_info = np.float(0)\n",
    "\n",
    "        # feature on which we goona split\n",
    "        feature = X[:, f_i] # feature corresponds to index f_i\n",
    "        Y_left = Y[np.where(feature <= boundry)]\n",
    "        Y_right = Y[np.where(feature > boundry)]\n",
    "\n",
    "        if len(Y_left) != 0:\n",
    "            split_info += np.float( -1*np.float(len(Y_left)/size) * np.log2(len(Y_left)/size) ) \n",
    "        if len(Y_right) != 0:\n",
    "            split_info += np.float( -1*np.float(len(Y_right)/size) * np.log2(len(Y_right)/size) )\n",
    "\n",
    "        return split_info\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "        Fit function learns all parameters of a node which we use during predict function\n",
    "    '''\n",
    "    def fit(self,X, Y, features, level=0):\n",
    "        self.level = level\n",
    "        \n",
    "        # Find current entropy\n",
    "        entropy_current = self.entropy_node(Y)\n",
    "\n",
    "        # Base case\n",
    "        # If node is pure, \n",
    "        if len(set(Y))==1 :\n",
    "            pass\n",
    "        # If no feature left to split\n",
    "        elif len(features) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            # Find max info_gain\n",
    "            max_gain_ratio = -10000 # Initially , it will be changed later\n",
    "            max_gain_ratio_feature = features[0]\n",
    "            X_feature_boundry = -1 # initially, we change this inside loop\n",
    "\n",
    "            # check info_gain for each feature\n",
    "            for f in features:\n",
    "                '''\n",
    "                    Since in Iris dataset all features have continuous data. So we need to find a boundry\n",
    "                    To find an boundry we first sort feature data, and then try boundry as middle value b/w 2 consecutive data\n",
    "                '''\n",
    "                X_feature = X[:, feature_indices[f]]\n",
    "                X_feature.sort()\n",
    "             \n",
    "                # Try different boundries\n",
    "                X_len = len(X_feature)\n",
    "                if X_len == 1:\n",
    "                    continue\n",
    "\n",
    "                for i in range(X_len-1):\n",
    "                    curr_boundry = np.float(X_feature[i]+X_feature[i+1])/2\n",
    "                    info_after_split_f = self.info_fi(X, Y, feature_indices[f], curr_boundry)\n",
    "                    info_gain_f = entropy_current - info_after_split_f\n",
    "                    split_info_f = self.find_split_info(X, Y, feature_indices[f], curr_boundry)\n",
    "                    gain_ratio_f = 0\n",
    "                    try:\n",
    "                        gain_ratio_f = np.float(info_gain_f/split_info_f)\n",
    "                    except:\n",
    "                        gain_ratio_f = 0\n",
    "\n",
    "                    if gain_ratio_f>max_gain_ratio:\n",
    "                        max_gain_ratio = gain_ratio_f\n",
    "                        max_gain_ratio_feature = f\n",
    "                        X_feature_boundry = curr_boundry\n",
    "\n",
    "                        \n",
    "            self.max_gain_ratio_feature = max_gain_ratio_feature\n",
    "            self.X_feature_boundry = X_feature_boundry\n",
    "            \n",
    "            # Now split on basis of max_gain_ratio_feature and X_feature_boundry and call on two sides of boundry\n",
    "            # 1st remove this max_gain_ratio_feature from current features list\n",
    "            new_features = []\n",
    "            for f in features:\n",
    "                if f != max_gain_ratio_feature:\n",
    "                    new_features.append(f)\n",
    "\n",
    "            f_i = feature_indices[max_gain_ratio_feature] # bcz this index(f_i) gives us the exact column correspond to required featue \n",
    "            f_i_feature = X[:, f_i] # feature corresponds to index f_i\n",
    "\n",
    "            # Recursive calls on 2 sides of boundry\n",
    "            X_left = X[np.where(f_i_feature <= X_feature_boundry)]\n",
    "            Y_left = Y[np.where(f_i_feature <= X_feature_boundry)]\n",
    "            #left decision tree\n",
    "            DT_left = DecisionTree() \n",
    "            DT_left.fit(X_left, Y_left, new_features, level+1)\n",
    "            self.left = DT_left # Attach on left side\n",
    "            \n",
    "            X_right = X[np.where(f_i_feature > X_feature_boundry)]\n",
    "            Y_right = Y[np.where(f_i_feature > X_feature_boundry)]\n",
    "            # right decision tree\n",
    "            DT_right = DecisionTree()\n",
    "            DT_right.fit(X_right, Y_right, new_features, level+1)\n",
    "            self.right = DT_right # Attach on right side\n",
    "            \n",
    "            \n",
    "            \n",
    "    def predict(self, X , Y, features):\n",
    "        # Print some info as required\n",
    "        print(\"Level\",self.level) \n",
    "\n",
    "        # store different classes possible with its frequency\n",
    "        class_freq = {}\n",
    "        for c in Y:\n",
    "            class_freq[c] = class_freq.get(c,0) + 1\n",
    "        \n",
    "        output = None\n",
    "        max_ = 0\n",
    "        for c in class_freq:\n",
    "            if class_freq[c] != 0:\n",
    "                print(\"Count of\",c,\"=\",class_freq[c])\n",
    "                if max_ < class_freq[c]:\n",
    "                    max_ = class_freq[c]\n",
    "                    output = c\n",
    "\n",
    "        # Find current entropy\n",
    "        entropy_current = 0\n",
    "        if len(set(Y)) > 1:\n",
    "            entropy_current = self.entropy_node(Y)\n",
    "\n",
    "        \n",
    "        if entropy_current==0:\n",
    "            print(\"Current Entropy is =\",entropy_current)\n",
    "            print(\"Reached Leaf Node\")\n",
    "        elif self.max_gain_ratio_feature is None :\n",
    "            print(\"Current Entropy is =\",entropy_current)\n",
    "            print(\"Reached Leaf Node and output is majority class i.e class\", output)\n",
    "        else:\n",
    "            print(\"Current Entropy is =\",entropy_current)\n",
    "                            \n",
    "            # boundry value on which max_gain_ratio_feature is splitted\n",
    "            curr_boundry = self.X_feature_boundry \n",
    "            \n",
    "            # feature to split\n",
    "            f_split = self.max_gain_ratio_feature \n",
    "            \n",
    "            info_after_split_f = self.info_fi(X, Y, feature_indices[f_split], curr_boundry)\n",
    "            \n",
    "            # Info_gain after splitting max_gain_ratio_feature with boundry X_feature_boundry\n",
    "            info_gain_f = entropy_current - info_after_split_f\n",
    "            \n",
    "            split_info_f = self.find_split_info(X, Y, feature_indices[f_split], curr_boundry)\n",
    "            \n",
    "            # gain ratio = (Info_gain / split_info)\n",
    "            gain_ratio_f = 0\n",
    "            try:\n",
    "                gain_ratio_f = np.float(info_gain_f/split_info_f)\n",
    "            except:\n",
    "                gain_ratio_f = 0\n",
    "                \n",
    "            print(\"Splitting on feature\",f_split, \"<=\",curr_boundry, \"with gain ratio\", gain_ratio_f)\n",
    "\n",
    "            # Now split on basis of f_split and curr_boundry and call on two sides of boundry\n",
    "            # 1st remove this max_gain_ratio_feature from current features list\n",
    "            new_features = []\n",
    "            for f in features:\n",
    "                if f != f_split:\n",
    "                    new_features.append(f)\n",
    "\n",
    "            f_i = feature_indices[f_split] # bcz this index(f_i) gives us the exact column correspond to required featue\n",
    "            f_i_feature = X[:, f_i] # feature corresponds to index f_i\n",
    "            \n",
    "            if self.left is not None:\n",
    "                X_left = X[np.where(f_i_feature <= curr_boundry)]\n",
    "                Y_left = Y[np.where(f_i_feature <= curr_boundry)]\n",
    "                print()\n",
    "                self.left.predict(X_left, Y_left, new_features)\n",
    "                \n",
    "            if self.right is not None:\n",
    "                X_right = X[np.where(f_i_feature > curr_boundry)]\n",
    "                Y_right = Y[np.where(f_i_feature > curr_boundry)]\n",
    "                print()\n",
    "                self.right.predict(X_right, Y_right, new_features)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create object of DecisionTree class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build tree (learn parameters) by calling fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(X_train,Y_train,features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### call predict function on test data to print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n",
      "Count of 0 = 13\n",
      "Count of 2 = 12\n",
      "Count of 1 = 20\n",
      "Current Entropy is = 1.545990246502223\n",
      "Splitting on feature petal length (cm) <= 6.0 with gain ratio 0.3391845278504918\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 13\n",
      "Count of 2 = 10\n",
      "Count of 1 = 20\n",
      "Current Entropy is = 1.524786634179887\n",
      "Splitting on feature sepal length (cm) <= 4.35 with gain ratio 0\n",
      "\n",
      "Level 2\n",
      "Current Entropy is = 0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 13\n",
      "Count of 2 = 10\n",
      "Count of 1 = 20\n",
      "Current Entropy is = 1.524786634179887\n",
      "Splitting on feature sepal width (cm) <= 2.2 with gain ratio 0.16405500703523648\n",
      "\n",
      "Level 3\n",
      "Count of 1 = 1\n",
      "Current Entropy is = 0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 3\n",
      "Count of 0 = 13\n",
      "Count of 2 = 10\n",
      "Count of 1 = 19\n",
      "Current Entropy is = 1.53432646944219\n",
      "Splitting on feature petal width (cm) <= 0.15000000000000002 with gain ratio 0.25397617232567776\n",
      "\n",
      "Level 4\n",
      "Count of 0 = 1\n",
      "Current Entropy is = 0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 4\n",
      "Count of 0 = 12\n",
      "Count of 2 = 10\n",
      "Count of 1 = 19\n",
      "Current Entropy is = 1.529516548204532\n",
      "Reached Leaf Node and output is majority class i.e class 1\n",
      "\n",
      "Level 1\n",
      "Count of 2 = 2\n",
      "Current Entropy is = 0\n",
      "Reached Leaf Node\n"
     ]
    }
   ],
   "source": [
    "dt.predict(X_test,Y_test,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n",
      "Count of 0 = 37\n",
      "Count of 2 = 38\n",
      "Count of 1 = 30\n",
      "Current Entropy is = 1.5773146804529516\n",
      "Splitting on feature petal length (cm) <= 6.0 with gain ratio 0.23751386521103268\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 36\n",
      "Count of 2 = 38\n",
      "Count of 1 = 24\n",
      "Current Entropy is = 1.5577950932979914\n",
      "Splitting on feature sepal length (cm) <= 4.35 with gain ratio 0.18107094736685828\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 1\n",
      "Current Entropy is = 0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of 2 = 38\n",
      "Count of 1 = 24\n",
      "Count of 0 = 35\n",
      "Current Entropy is = 1.5588277790815832\n",
      "Splitting on feature sepal width (cm) <= 2.2 with gain ratio 0.09865463918569896\n",
      "\n",
      "Level 3\n",
      "Count of 2 = 1\n",
      "Count of 1 = 1\n",
      "Current Entropy is = 1.0\n",
      "Splitting on feature petal width (cm) <= 0.1 with gain ratio 0\n",
      "\n",
      "Level 4\n",
      "Count of 2 = 1\n",
      "Count of 1 = 1\n",
      "Current Entropy is = 1.0\n",
      "Reached Leaf Node and output is majority class i.e class 2\n",
      "\n",
      "Level 4\n",
      "Current Entropy is = 0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 3\n",
      "Count of 0 = 35\n",
      "Count of 1 = 23\n",
      "Count of 2 = 37\n",
      "Current Entropy is = 1.5559966071661449\n",
      "Splitting on feature petal width (cm) <= 0.15000000000000002 with gain ratio 0.18160828513505606\n",
      "\n",
      "Level 4\n",
      "Count of 0 = 1\n",
      "Current Entropy is = 0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 4\n",
      "Count of 1 = 23\n",
      "Count of 0 = 34\n",
      "Count of 2 = 37\n",
      "Current Entropy is = 1.5570842121021697\n",
      "Reached Leaf Node and output is majority class i.e class 2\n",
      "\n",
      "Level 1\n",
      "Count of 1 = 6\n",
      "Count of 0 = 1\n",
      "Current Entropy is = 0.5916727785823274\n",
      "Splitting on feature sepal width (cm) <= 3.95 with gain ratio 0.3544794366054113\n",
      "\n",
      "Level 2\n",
      "Count of 1 = 5\n",
      "Current Entropy is = 0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 1\n",
      "Count of 1 = 1\n",
      "Current Entropy is = 1.0\n",
      "Splitting on feature sepal length (cm) <= 7.800000000000001 with gain ratio 1.0\n",
      "\n",
      "Level 3\n",
      "Count of 0 = 1\n",
      "Current Entropy is = 0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 3\n",
      "Count of 1 = 1\n",
      "Current Entropy is = 0\n",
      "Reached Leaf Node\n"
     ]
    }
   ],
   "source": [
    "dt.predict(X_train,Y_train,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
