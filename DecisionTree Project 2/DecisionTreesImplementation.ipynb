{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTree for OR operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True False]\n",
      " [False  True]\n",
      " [False False]\n",
      " [ True  True]]\n",
      "[ True  True False  True]\n"
     ]
    }
   ],
   "source": [
    "features = ['X1', 'X2']\n",
    "classes = np.array([True, False])\n",
    "\n",
    "X = np.array([[True, False], [False, True], [False, False], [True, True]])\n",
    "Y = np.array([True, True, False, True])\n",
    "\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_node(Y):\n",
    "    total_freq = len(Y)\n",
    "    \n",
    "    # store different classes possible with its frequency\n",
    "    class_freq = {}\n",
    "    for c in Y:\n",
    "        class_freq[c] = class_freq.get(c,0) + 1\n",
    "        \n",
    "    # Now we calculate entropy or info required\n",
    "    info_req =  0\n",
    "    for k in class_freq:\n",
    "        prob_k = class_freq[k]/total_freq\n",
    "        if prob_k!=0:\n",
    "            info_req += (-1 * prob_k * np.log2(prob_k))\n",
    "    return info_req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate entropy(info required) after spliting on basis of feature having index f_i\n",
    "\n",
    "def info_fi(X, Y, f_i):\n",
    "    entropy_after_split = 0\n",
    "    \n",
    "    # feature on which we goona split\n",
    "    feature = X[:, f_i] # feature corresponds to index f_i\n",
    "    f_set = set(feature)\n",
    "    for c in f_set:\n",
    "        Y_c = Y[np.where(feature == c)]\n",
    "        entropy_after_split += ( (len(Y_c)/len(Y)) * entropy_node(Y_c) )\n",
    "        \n",
    "    return entropy_after_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate split_info of feature having index f_i\n",
    "\n",
    "def find_split_info(X, Y, f_i):\n",
    "    split_info = 0\n",
    "    size = len(Y)\n",
    "    \n",
    "    # feature on which we goona split\n",
    "    feature = X[:, f_i] # feature corresponds to index f_i\n",
    "    f_set = set(feature)\n",
    "    for c in f_set:\n",
    "        Y_c = Y[np.where(feature == c)]\n",
    "        split_info += ( -1*(len(Y_c)/size) * np.log2(len(Y_c)/size) )\n",
    "        \n",
    "    return split_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTree(X, Y, features, level=0):\n",
    "    # Print some info as required\n",
    "    print(\"Level\",level) \n",
    "    \n",
    "    # store different classes possible with its frequency\n",
    "    class_freq = {}\n",
    "    for c in Y:\n",
    "        class_freq[c] = class_freq.get(c,0) + 1\n",
    "    \n",
    "    for c in class_freq:\n",
    "        if class_freq[c] != 0:\n",
    "            print(\"Count of\",c,\"=\",class_freq[c])\n",
    "    \n",
    "    # Find current entropy\n",
    "    entropy_current = entropy_node(Y)\n",
    "    \n",
    "    \n",
    "    # Base case\n",
    "    # If node is pure, \n",
    "    if len(set(Y))==1 :\n",
    "        print(\"Current Entropy is = 0.0\")\n",
    "        print(\"Reached Leaf Node\")\n",
    "    \n",
    "    # If no feature left to split\n",
    "    elif len(features) == 0:\n",
    "        print(\"Current Entropy is =\",entropy_current)\n",
    "        print(\"Reached Leaf Node\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Current Entropy is =\",entropy_current)\n",
    "        \n",
    "        # Find max info_gain\n",
    "        max_gain_ratio = 0\n",
    "        max_gain_ratio_feature = features[0]\n",
    "        \n",
    "        # check info_gain for each feature\n",
    "        for f in features:\n",
    "            info_after_split_f = info_fi(X, Y, feature_indices[f])\n",
    "            info_gain_f = entropy_current - info_after_split_f\n",
    "            split_info_f = find_split_info(X, Y, feature_indices[f])\n",
    "            gain_ratio_f = info_gain_f/split_info_f\n",
    "            \n",
    "            if gain_ratio_f>max_gain_ratio:\n",
    "                max_gain_ratio = gain_ratio_f\n",
    "                max_gain_ratio_feature = f\n",
    "                \n",
    "        print(\"Splitting on feature\",max_gain_ratio_feature, \"with gain ratio\", max_gain_ratio)\n",
    "        \n",
    "        # Now split on basis of max_gain_ratio_feature and call recursively\n",
    "        # 1st remove this max_gain_ratio_feature from current features list\n",
    "        new_features = []\n",
    "        for f in features:\n",
    "            if f != max_gain_ratio_feature:\n",
    "                new_features.append(f)\n",
    "        #features = new_features # Now features list doesn't contain max_gain_ratio_feature\n",
    "                \n",
    "        f_i = feature_indices[max_gain_ratio_feature] # bcz this index(f_i) gives us the exact column correspond to required featue \n",
    "        f_i_feature = X[:, f_i] # feature corresponds to index f_i\n",
    "        f_set = set(f_i_feature)\n",
    "        for c in f_set:\n",
    "            X_c = X[np.where(f_i_feature == c)]\n",
    "            Y_c = Y[np.where(f_i_feature == c)]\n",
    "            \n",
    "            # Recursive calls\n",
    "            print()\n",
    "            DecisionTree(X_c, Y_c, new_features, level+1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n",
      "Count of True = 3\n",
      "Count of False = 1\n",
      "Current Entropy is = 0.8112781244591328\n",
      "Splitting on feature X1 with gain ratio 0.31127812445913283\n",
      "\n",
      "Level 1\n",
      "Count of True = 1\n",
      "Count of False = 1\n",
      "Current Entropy is = 1.0\n",
      "Splitting on feature X2 with gain ratio 1.0\n",
      "\n",
      "Level 2\n",
      "Count of False = 1\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of True = 1\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of True = 2\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Store feature names and its corresponding indices initially. Otherwise in every recusive call we need to make new X ndarray\n",
    "    which increases complexity.\n",
    "'''\n",
    "feature_indices = {}\n",
    "for i in range(len(features)):\n",
    "    feature_indices[features[i]] = feature_indices.get(features[i],0) + i\n",
    "    \n",
    "# Call Decision Tree\n",
    "DecisionTree(X,Y,features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
